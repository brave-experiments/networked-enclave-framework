\section{Framework Design}
\label{sec:design}

This section introduces the design of our framework.  We start by laying out the
involved parties and their respective trust assumptions
(\S~\ref{sec:trust-assumptions}), followed by an overview of our system
(\S~\ref{sec:overview}).  We then discuss the two major aspects of our
framework: the build system (\S~\ref{sec:build-system}) and the framework
itself~(\S~\ref{sec:framework}).

\subsection{Trust Assumptions}
\label{sec:trust-assumptions}

Our setting has three participants that have the following trust assumptions:

\begin{enumerate}
    \item The \emph{service provider} runs a service for its clients.  As part
      of its operations, the service provider wants to process sensitive client
      information.

    \item The \emph{client} is a user of the service provider.  It does not
      trust the service provider with its sensitive information and demands
      verifiable guarantees that the service provider will never see the
      client's sensitive information in plaintext.

    \item The \emph{enclave provider} makes available enclaves to the service
      provider.  Both the client and the service provider trust that the enclave
      provider's enclaves have the advertised security attributes of integrity,
      confidentiality, and verifiability.
\end{enumerate}



\subsection{Design Overview}
\label{sec:overview}

We begin with a short, informal overview of our system to provide intuition.
The subsequent sections are going to elaborate on this high-level picture.
The life cycle of an enclave application that uses our framework involves five
steps:

\begin{enumerate}
    \item The service provider implements a new service with the intention of
      running it in an enclave.  Once the implementation is finished, the
      service provider publishes the source code for its clients to audit, and
      runs the code in an enclave.  After booting, the enclave automatically
      obtains a CA-signed TLS certificate.

    \item Users audit the source code.  Once a user has convinced herself that
      the code is free of bugs, she compiles the code using the framework's
      deterministic build system, resulting in an image checksum.

    \item The client establishes an end-to-end encrypted network connection with
      the enclave, facilitated by the EC2 host blindly forwarding encrypted
      bytes.  Right \emph{after} establishing the TLS connection but
      \emph{before} revealing any sensitive information, the client provides a
      nonce and asks the enclave for an attestation document.

    \item The enclave receives the nonce and asks its hypervisor to generate an
      attestation document that should contain the client-provided nonce
      \emph{and} the fingerprint of the enclave's CA-signed certificate in
      addition to the usual image measurement checksums.  The
      resulting attestation document is returned to the client.

    \item The client performs various checks (see \S~\ref{sec:attestation} for
      details) and trusts the enclave if all checks pass.  The client is then
      convinced that it's communicating with the code that the user audited in
      the previous steps, and is willing to reveal her sensitive information to
      the enclave.
\end{enumerate}

% As illustrated in Figure~\ref{fig:overview}, clients submit their IP address
% to the enclave, which is run by the enclave provider but its code comes from
% the service provider.  It's not possible to talk to the enclave directly
% because all communication happens via the (untrustworthy) parent EC2
% instance, which is why we introduce a TCP proxy (run by the infrastructure
% provider), which prevents the EC2 instance from seeing the client's IP
% address.  The client establishes a TLS session that is terminated inside the
% enclave and can therefore be sure that its IP address is sent to the enclave,
% and the enclave only.

% After the enclave received the client's address, it responds with a nonce.
% The enclave does not know if the client reported its real address or a fake
% address, so it must actively verify the address, but it cannot connect to the
% client directly because that would allow the EC2 instance to see the client's
% IP address.  To solve this problem, we make the enclave talk to the client
% via a VPN node.  Upon connection establishment, the client must send the
% nonce it received from the enclave in the previous step.  Once the enclave
% recognizes the nonce, it rests assured that the client really does control
% the IP address it submitted earlier.

% In the final step, the enclave anonymizes the client's IP address and
% forwards it to its back end.

\subsection{The Reproducible Build System}
\label{sec:build-system}

% Why it's not a big deal that only some users can audit our code.
Only a small subset of the users will be skilled enough programmers to audit
the enclave's code for bugs.  We expect non-technical users to trust that other
users---or perhaps professional code audit companies---have studied the code
and pointed out potential bugs.  Once a user has convinced herself of the code's
correctness, she compiles the code to arrive at an image ID.  Crucially, we
need a \emph{deterministic mapping} between the code and its corresponding
image ID because the service provider and clients must agree on the image ID
that's running in the enclave.

The popular Docker tool does not offer a deterministic mapping because, among
other things, Docker records timestamps in its build process, causing subsequent
builds of identical code to result in different image IDs.\footnote{In essence,
a Docker image is merely a file system.  A Docker image is reproducible when
separate build processes arrive at the exact same file system, including meta
data like timestamps.}  To obtain reproducible builds, we replace Docker with
the kaniko tool~\cite{kaniko}.  Kaniko's main purpose is to build container
images from a Dockerfile while itself in a container, but we use kaniko because
it can do so reproducibly.  As long as the client and service provider use the
same enclave source code, Go version, and kaniko version, they can build
identical images---even when compiling the code on different platforms, like Mac
OS and Linux.  Equipped with a locally-compiled Docker image ID, the client is
now ready to interact with the enclave.

\subsection{Framework Components}
\label{sec:framework}

Having discussed how the client and service provider can independently compile
identical Docker image IDs, we now turn to the specific components of our
framework.  The following sections discuss how the framework
seeds its empty entropy pool (\S~\ref{sec:entropy});
how it communicates with the outside world (\S~\ref{sec:networking});
how it obtains a CA-signed certificate (\S~\ref{sec:cert});
how we facilitate remote attestation (\S~\ref{sec:attestation});
how enclaves can share their key material to allow for horizontal scaling (\S~\ref{sec:sync});
how to thwart side-channel attacks (\S~\ref{sec:side-channels}),
how to ingest secrets (\S~\ref{sec:secrets}), and
concludes with a simple example (\S~\ref{sec:example}).

\subsubsection{Seeding the Entropy Pool}
\label{sec:entropy}

Like many virtual machines, a Nitro enclave is an entropy-starved, sterile
environment without any periphery devices that help the kernel seed its entropy
pool.  To work around that, the Nitro hypervisor can provide randomness for the
enclave to seed its entropy pool.  Our framework automatically takes advantage
of that when it first starts (step~\ding{202} in Figure~\ref{fig:networking}),
so application developers never encounter function calls that block because of
insufficient randomness.

\subsubsection{Enabling Networking}
\label{sec:networking}

Recall from Section~\ref{sec:nitro} that Nitro enclaves have no networking
interface and are only able to communicate with their respective EC2 host.  Our
framework therefore needs proxying code that runs on the EC2 host and forwards
networking packets between clients and the enclave.  Figure~\ref{fig:networking}
illustrates the design of the networking architecture.

\begin{figure}[t]
\centering
\input{sections/figures/network-architecture}
\caption{Upon bootstrapping, the application first asks the hypervisor for
  randomness to seed its entropy pool (\ding{202}), followed by initiating an
  ACME session to obtain a Let's Encrypt-signed certificate (\ding{203}), after
  which Let's Encrypt probes the enclave and issues the certificate
  (\ding{204}).  Afterwards, clients can establish HTTPS connections with the
  enclave (\ding{205}) and the enclave can forward data to its back end
  (\ding{206}).  All of the application's ingress and egress  traffic is routed
  over a TCP proxy that translates between AF\_INET and AF\_VSOCK.  Egress
  traffic reaches the Internet via a SOCKS proxy.}
\label{fig:networking}
\end{figure}

When the enclave first starts, it fetches a CA-signed certificate from Let's
Encrypt (cf. \S~\ref{sec:cert}) using the ACME protocol~\cite{acme-protocol},
which allows for the automated issuance of certificates.  To do so, it first
connects to Let's Encrypt's infrastructure via a SOCKS proxy (step~\ding{203}).
Let's Encrypt does not publish its endpoints' IP addresses, which is why we
cannot use point-to-point connections and have to rely on the flexibility of a
SOCKS proxy.\footnote{Our SOCKS proxy is available at
\url{https://github.com/brave-intl/bat-go/tree/nitro-utils/nitro-shim/tools/socksproxy}
and our TCP proxy is available at
\url{https://github.com/brave-experiments/viproxy}.} Let's Encrypt then verifies
that the enclave can answer queries on behalf of the requested domain name, and
issues a certificate to the enclave (step~\ding{204}).

Clients establish end-to-end encrypted TLS sessions to the enclave with the help
of a TCP proxy on the EC2 host that translates between AF\_INET (traditional,
IP-based sockets) and AF\_VSOCK (VSOCK-based sockets).  Note that the EC2 host
is blindly forwarding encrypted bytes, and cannot see what data the client and
the enclave are exchanging (step~\ding{205}).  In an optional, final step, the
enclave can send data to a back end (step~\ding{206}).

In case of an enclave compromise, we don't want the application to be able to
leak data to an attacker-controlled endpoint via the SOCKS proxy, which is why
we use an allow list on the SOCKS proxy.  It is the service provider's
responsibility to maintain the allow list.\footnote{Note that clients are unable
to remotely verify that the service provider correctly configured an allow list
because the SOCKS proxy runs on the EC2 host and not in the enclave
application.}  In a minimal application, the allow list consists of two
endpoints:

\begin{enumerate}
    \item The domain name acme-v02.api.letsencrypt.org to interact with Let's
      Encrypt's infrastructure.
    \item The IP addresses or domain names of whatever back end machine the
      enclave needs to talk to.
\end{enumerate}

If an attacker discovered a remote code execution bug in the enclave application
and uses the bug to make the enclave establish a connection to any domain name
that is not part of the allow list, like evil.com, the SOCKS proxy is going to
reject the connection.

\subsubsection{End-to-end Secure Channel}
\label{sec:cert}

Having established how the enclave can send and receive network packets, we now
turn our attention to secure channels; specifically: how can a client rest
assured that it is talking to audited enclave application code, without taking
advantage of an existing trust relationship?

The application can register arbitrary HTTP handlers that are accessible to the
outside world to process data and provide services.
Our framework provides a secure channel based on HTTPS, i.e., we make it
possible for a client to establish an HTTPS connection with an enclave in a way
that the TLS connection is terminated inside the enclave. Clients can use this
channel to access the application without revealing data to third-party
observers, or to the service provider, outside of the specific ways the enclave
application permits.

Once the enclave has initialized its entropy pool, it obtains an HTTPS
certificate that allows clients to establish end-to-end encrypted session with
the enclave.  Crucially, the HTTPS certificate \emph{lives and dies} inside the
enclave and its private key cannot be extracted (or injected) by the service
provider because enclaves are sealed at runtime.  Our framework allows for both
the creation of a self-signed or a CA-signed certificate.  If a self-signed
certificate is desired, the framework creates and signs a certificate for a
given FQDN.  To get a CA-signed certificate, the framework uses Let's Encrypt's
ACME protocol because it allows for the generation of a certificate with no
human interaction.  In that case, the enclave initiates an HTTP-01
challenge\footnote{ACME supports multiple challenge types, with HTTP-01 being
the most common one~\cite{http-01}.  In HTTP-01, the ACME infrastructure
provides the client with a token, which must subsequently be available at a
pre-defined path on the client's Web server.} connection with Let's Encrypt's
infrastructure via our SOCKS proxy (cf.  Figure~\ref{fig:networking}), and
subsequently expects an incoming connection from Let's Encrypt to port 80, which
the EC2 host forwards to the enclave.  Note that the EC2 host can also obtain a
CA-signed certificate for the same FQDN because the enclave and the EC2 host
share an IP address.  This is however of little use to the EC2 host as we will
show in the next section.

\subsubsection{Remote Attestation}
\label{sec:attestation}

By default, Nitro enclaves only allow for local attestation.  We now discuss how
we allow clients to remotely retrieve and verify an enclave's attestation.
After the client establishes an HTTPS connection with the enclave, it needs to
know that (\emph{i}) the TLS connection it just established is terminated inside
the enclave (instead of by the EC2 host) and (\emph{ii}) the enclave is running
the code that the user audited in the previous step.  To that end, the client
requests the enclave's \emph{attestation document}---a hypervisor-signed
document that attests to the container image ID that the enclave is running.
Enclaves communicate with the hypervisor via the ioctl system call, which makes
use of /dev/nsm, a device that is only available inside a Nitro enclaves.  To
request an attestation document, the client provides a \emph{nonce}---a 160-bit
random value---whose purpose is to prevent the service provider from replaying
outdated attestation documents.  Phrased differently, the client provides a
nonce to convince itself that it's talking to a live enclave.  In practice,
clients make the following HTTP request to obtain an enclave's attestation
document:

\begin{lstlisting}[numbers=none,basicstyle=\small\ttfamily]
GET /attestation?nonce=8083...23b7 HTTP/1.1
\end{lstlisting}

The enclave receives the request through the TCP proxy, asks the hypervisor to
include the nonce \emph{and} the fingerprint of the enclave's X.509 certificate
in the attestation document, and sends the resulting attestation document to the
client.  By asking the hypervisor to include the certificate fingerprint in the
attestation document, we effectively bind a TLS session to an enclave, which is
key to assuring the client that it is in fact talking to an enclave.  Upon
receiving the attestation document, the client then verifies the following in
order:

\begin{enumerate}
    \item The attestation document is signed by the AWS PKI whose public key is
      known to all parties.
    \item The challenge nonce is part of the attestation document.
    \item The fingerprint of the enclave's X.509 certificate from the TLS session is part of the
      attestation document.
    \item The enclave's image ID is identical to the image ID that the client
      compiled locally.
\end{enumerate}

Only if all four conditions hold is the client convinced that it is talking to
an enclave running the code that the client audited, and that the TLS connection
is terminated by the enclave.  Note that the EC2 host is able to intercept HTTPS
connections with its own CA-signed certificate but clients will only trust the
EC2 host if (and only if) it can present an attestation document that is valid
for the enclave image, which it can't because it is unable to spoof the AWS PKI
signature that authenticates the attestation document.  The only way for the EC2
host to obtain such an attestation document is to spawn an enclave that runs the
exact code that the client is expecting---and it already is doing exactly that.
Now that the client has established a trust relationship with the enclave, it is
ready to reveal sensitive information to the enclave.

While attestation documents can be generated quickly and in rapid succession
(cf. \S~\ref{sec:attestation-performance}), they do require an extra round trip
between the client and the enclave before the client is willing to reveal
sensitive information: the client first provides a nonce to the enclave, the
enclave responds with an attestation document, and only after the client
verified the document is it willing to reveal its sensitive information.  To
eliminate unnecessary future round trips, clients should use TLS session
resumption once they have verified the enclave's attestation document.  The
service provider is unable to see the secret key material that protects the TLS
session between client and enclave, so it is safe to re-use a once-established
TLS session and forgo the unnecessary round trip.

\paragraph{Manual Client-side Verification}

In general, we envision that remote attestation is handled transparently by
client software, without involving the user.  In some cases however, users may
wish to manually verify an enclave.  Even for developers, remote attestation is
a complex process that is difficult to understand and work with.  To make
matters more complex, in our setting end users are expected to conduct remote
attestation.  We therefore made a careful effort to abstract away technical
details.  A user wishing to remotely verify an enclave essentially asks herself
``does the enclave that's exposed at a given URL run the source code that I just
audited?''  We built a tool set that reduces the process to the running of a
Makefile,\footnote{The source code is available at:
\url{https://github.com/brave-experiments/verify-enclave}.} i.e.:

\begin{lstlisting}[numbers=none,basicstyle=\small\ttfamily]
$ make verify CODE="/path/to/enclave/code/" \
              ENCLAVE="https://example.com/attest"
\end{lstlisting}

The first argument, \texttt{CODE}, points to the directory containing the source
code that the enclave is supposedly running, and the user audited.  The second
variable, \texttt{ENCLAVE}, points to the URL endpoint of the enclave that the
user's client is connecting to.  When the user runs this command, the Makefile
deterministically compiles the given source code to obtain its image ID, asks
the enclave for an attestation document, verifies the document, and ensures that
the attestation document is for the image ID that was compiled in the first
step.  If all checks pass, the tool informs the user accordingly.

\subsubsection{Sharing Key Material}
\label{sec:sync}

Recall that enclaves are essentially sealed at runtime, preventing anyone
(including both Amazon and the service provider) from extracting key material
that was generated inside the enclave.  While this is a desirable property, it
complicates horizontal scaling.  If a single enclave is unable to handle the
service provider's traffic load, one must scale horizontally by starting new
enclaves.  In some applications, it is unacceptable for each enclave to use
distinct key material.  Instead, enclaves must synchronize their key material,
so they appear to the outside world like a single machine.

While it is possible to build key synchronization using tools like the AWS key
management service (KMS),\footnote{One could encrypt the keys using a KMS
policy that dictates that only enclaves are allowed to decrypt it, and store
the encrypted key in a location that all enclaves can access, e.g., an S3
bucket.} we refrain from using KMS because there is no straightforward way for
users to verify that the service provider is using KMS as promised.  We
therefore devise a new protocol that enables key synchronization without having
to rely on external services.

We solve this problem in two steps: \emph{discovery} and
\emph{synchronization}.  First, enclaves must be able to discover each other,
i.e., learn each other's IP addresses.  Then, enclaves can establish
connections to each other and initiate key synchronization.  Our protocol
dictates that when a new enclave bootstraps, it first tries to discover
already-existing enclaves.  If there are none, the enclave knows that it is the
``origin'' enclave; it generates new key material and is prepared to share it
with future enclaves.  If however it discovers other enclaves, the new enclave
establishes a connection to another randomly-chosen enclave and initiates key
synchronization.  Crucially, key material is only shared after \emph{mutual
attestation}, i.e., the original and subsequent enclaves verify each other, and
only exchange key material if remote attestation succeeds.  Key synchronization
happens in three steps, as illustrated in Figure~\ref{fig:key-synchronization}.

\begin{figure}[t]
  \centering
  \input{sections/figures/key-synchronization}
  \caption{When a new enclave bootstraps, it discovers existing enclaves by
    obtaining the DNS SRV record for its own, hard-coded FQDN.  The enclave then
    initiates key synchronization by first requesting a nonce.  Then, the new
    enclave requests the origin enclave's key material by submitting its own
    attestation document, followed by receiving the origin enclave's attestation
    document, which contains encrypted key material.}
  \label{fig:key-synchronization}
\end{figure}

\begin{enumerate}

  \item Once a new enclave is spun up, it queries the DNS SRV record of the FQDN
    that is hard-coded in the enclave, e.g., example.com.  The DNS resolver will
    return the record, containing a list of enclaves that are already running and initialized.
    The new enclave picks a random enclave from the list and initiates key
    synchronization.

  \item The new enclave asks the existing enclave for a random nonce,
    $\textrm{nonce}_o$.  Each enclave caches $\textrm{nonce}_o$ for one
    minute.

  \item The new enclave now requests the key material from the existing enclave.
    As part of the request, it provides its attestation document that contains
    $\textrm{nonce}_o$ (to prove freshness to the existing enclave);
    $\textrm{nonce}_n$ (the existing enclave is expected to add this nonce to its
    attestation document); and $K_n$ (a public key to which the key material
    should be encrypted).  Upon receipt of the new enclave's attestation
    document, the existing enclave verifies the attestation document's signature
    and ensures that the new enclave is running the same code, i.e., the measured checksum
    value that uniquely identifies the enclave image is identical.  Once the existing
    enclave is convinced that it is dealing with a genuine new enclave, it
    creates an attestation document by including $\textrm{nonce}_n$ (to prove
    freshness to the new enclave) and $\textsf{Enc}(K_n, s)$---the key material
    $s$ is encrypted using the public key that the new enclave provided in the
    request.  Finally, the new enclave verifies the attestation document,
    decrypts the key material, and uses it to finish bootstrapping.

\end{enumerate}

% Security considerations.
Needless to say, the security of key synchronization is paramount.  As an
optional first layer of defense, synchronization should be configured to use a
private network segment limited to inter-enclave communication, such as the
internal network that is part of a Kubernetes cluster that is managing scaling
for the application.  While optional, Kubernetes is an attractive component in
our setting considering that enclaves are essentially compiled Docker images.
The second and main layer of defense is the fact that an enclave has to provide
a valid attestation document before the origin enclave reveals its key material.
As long as the origin enclave knows that an identical and authentic copy of
itself is asking for key material, it will readily provide it.

In their 2022 paper, Chen and Zhang present MAGE, a protocol that allows
enclaves to mutually verify each other without relying on a trusted third
party~\cite{Chen2022a}.  We could have built key synchronization on top of MAGE
but found that our setting is considerably simpler because only \emph{identical}
enclaves request each other's key material, eliminating the need for the more
flexible---but also more complex---MAGE protocol.

\subsubsection{Side-channel Attacks}
\label{sec:side-channels}

The enclave's parent EC2 host cannot see \emph{what} clients send to the enclave
but it can see \emph{how much} clients send and \emph{how long} it takes the
enclave to process data.  The EC2 host can exploit these side channels to learn
more about the client's confidential information and computation.  While such
side channels must be avoided, our framework is not the place to do so.
Instead, it is the application developer's responsibility to identify and
address this class of attacks.  Section~\ref{sec:applications} introduces two
applications and discusses side channel attacks in their respective setting.

Similarly out of scope are programming bugs in the enclave application.  Memory
corruption bugs may be more difficult to exploit in an enclave
application\footnote{The untrustworthy operating system (that may be under the
attacker's control) is prohibited by hardware to read the enclave application's
memory or registers in clear text, which forces the attacker to operate
blindly.} but Lee et al. adapted a return-oriented programming attack against
SGX to show that such attacks are practical~\cite{Lee2017a}.

\subsubsection{Ingesting Secrets}
\label{sec:secrets}

A key design requirement of our framework is that users can audit and verify the
code that is running inside an enclave, which means that the service provider is
unable to hide any software configuration from the user.  Service providers can
work around this shortcoming by implementing HTTP handlers that take as input
arbitrary data, and use it to update the enclave's state.  Consider a system
that takes as input client IP addresses, anonymizes them, and forwards the
anonymized addresses to a back end (cf. \S~\ref{sec:pseudonymization}).  The
service provider now wants to compare submitted IP addresses to a confidental
deny list.  However, if the deny list is hard-coded in the freely available
enclave application, it is readily visible to anyone.  The service provider can
solve this problem by adding to the enclave application a new HTTP handler that
takes as input the confidential data it seeks to protect from the users' eyes.
Once the enclave is running, the service provider loads the confidential data
at runtime, by calling the end point.  To prevent users from submitting bogus
data, the endpoint could hard-code the service provider's public key and only
accept data that carries a valid signature of the service provider's private
key.

This technique for ingesting secrets into an enclave's runtime is flexible---so
flexible, in fact, that the service provider could abuse it to ingest code at
runtime, which would nullify the enclave's verifiability requirement.  Vigilant
users would never trust an enclave whose code can change at runtime.  We
therefore argue that an HTTP handler for the purpose of injesting secrets must
be constrained to a point that only data of a well-defined type can be injested.

\subsubsection{An Example}
\label{sec:example}

Figure~\ref{fig:hello-world} illustrates an example of a simple ``hello world''
application.  The code initializes a new enclave struct (line 16), followed by
adding a handler that processes requests for \texttt{GET /hello-world} (line
24).  Finally, the application starts the enclave using a function call that
does not return (line 27).

\begin{figure}[t]
\begin{lstlisting}[language=go]
package main

import (
    "fmt"
    "log"
    "net/http"

    nitro "REDACTED"
)

func handler(w http.ResponseWriter, r *http.Request) {
    fmt.Fprintln(w, "hello world")
}

func main() {
    enclave := nitro.NewEnclave(
        &nitro.Config{
            FQDN:    "example.com",
            Port:    8443,
            UseACME: true,
            Debug:   false,
        },
    )
    enclave.AddRoute(http.MethodGet, "/hello-world", handler)
    if err := enclave.Start(); err != nil {
        log.Fatalf("Terminated: %v", err)
    }
}
\end{lstlisting}
\caption{An example of a simple enclave application which registers an HTTP GET
  handler for the path /hello-world (line 24) and, when accessed, responds with
  the string ``hello world'' in the response body (line 12).}
\label{fig:hello-world}
\end{figure}

The configuration object (line 17) consists of four fields.  The first field
determines the enclave's FQDN, which is required when obtaining an X.509
certificate.  The port is listened on by the enclave via the VSOCK interface.
It is the EC2 host's responsibility to forward incoming traffic to
this port.  The third field determines if the enclave should use Let's Encrypt's
ACME protocol to obtain a CA-signed certificate at runtime.
The last flag instructs the framework to print debug information.  Note that
this is only useful when the enclave is invoked in debug mode, which disables
remote attestation.
