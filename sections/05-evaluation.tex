\section{Evaluation}
\label{sec:evaluation}

We evaluate our enclave framework with respect to security
(\S~\ref{sec:security}), cost (\S~\ref{sec:cost}), and performance.  As for
performance, we study the rate at which one can generate attestation documents
(\S~\ref{sec:attestation-performance}), we present our performance test setup
(\S~\ref{sec:test-setup}), and measure end-to-end request latency
(\S~\ref{sec:end-to-end}).

\subsection{Security Considerations}
\label{sec:security}

There are three key components to the overall security of enclave applications;
(\emph{i}) Amazon's Nitro enclave system itself, (\emph{ii}) our framework, and
(\emph{iii}) the application that runs on top of our framework.

The very foundation of our framework's security lies in the soundness of the
design of Nitro enclaves.  While Amazon published the conceptual design, the
concrete hardware and software implementation remains confidential.  While
Amazon's decision to allocate physically separate resources to enclaves appears
promising, the coming years will tell if the implementation can resist the types
of attacks that have been plagueing SGX.  If we assume that Nitro enclaves are
acceptably secure, the next critical layer is our software framework.

A significant security aspect of our framework is its size; it is well
understood that complexity is the enemy of security.  Our framework counts less
than 700 lines of code and has four direct dependencies that are not maintained
by either us or the Go project.\footnote{The dependencies are chi~\cite{chi}
(provides an HTTP request router), nsm~\cite{nsm} (provides an interface to
interact with the Nitro hypervisor), vsock~\cite{vsock} (provides an API for the
VSOCK address family), and tenus~\cite{tenus} (provides an API to configure
Linux's networking devices).} Four is worse than zero, but it is still
manageable and reasonably easy to audit in its entirety.  We believe that our
choice of using Go and the deliberately small trusted computing base greatly
reduces---but not eliminates!---the attack surface.

The highest layer in the software stack is the enclave application itself.  The
biggest security threat are side channel attacks and programming bugs---both
unintentional and intentional.  It is the application developer's responsibility
to prevent side channel attacks and write bug-free code.  As we pointed out in
Section~\ref{sec:limitations}, programming bugs can be intentional, i.e., the
service provider may deliberately introduce bugs that leak sensitive
information, and hope that it won't get caught.

\subsection{Financial Cost}
\label{sec:cost}

Nitro enclaves do not incur any extra cost in addition to what the underlying
EC2 instance costs---they can be considered a ``free'' extension to EC2.  Nitro
enclaves are however only available for select types of EC2 instances because
they require their own CPU and a minimum amount of memory, and those instance
types are pricier than the lowest tier that AWS offers.

\phw{Add more details here.  Are we going to have to pay for egress traffic?
How much?}

We are currently working on deploying the IP address pseudonymization prototype
that we introduced in Section~\ref{sec:pseudonymization}.  We estimate that our
enclave is going to have to handle an average of 5,000 requests per minute,
coming from more than ten million clients.  Our test deployment uses a single c5.xlarge
EC2 instance in the U.S. East region which costs \$0.17 per hour to operate,
amounting to approximately \$125 per month.

\subsection{Attestation Documents}
\label{sec:attestation-performance}

The fetching of attestation documents is a critical part of our framework's
overall performance.  We wrote a stress test tool that requests as many
attestation documents as it can over sixty seconds.  The tool is essentially a
minimal enclave application that does nothing but requesting attestation
documents.  For each attestation document, we asked the hypervisor to include
an incrementing nonce, to avoid any speedups by caching.  We were able to
receive approximately 900 documents per second, with each request taking a
median of one millisecond ($s = 0.3\,\text{ms}$) to fetch the attestation
document.\footnote{We performed our measurements on a c5.xlarge EC2 instance
which comes with four CPUs and eight GiB of memory.}

\subsection{Test setup}
\label{sec:test-setup}

Next, we set out to measure the networking latency of the critical path, as
illustrated in Figure~\ref{fig:stress-test}.  In particular, we test the
latency of our TCP proxy, the VSOCK interface between EC2 and enclave, and a
minimal enclave application.
%
We measure latency in three separate setups, designed to help us understand how
much latency each component in our data flow adds:

\begin{figure}[t]
    \centering
    \input{sections/figures/stress-test}
    \caption{Our stress test tool tests the performance of our critical path,
    consisting of the TCP proxy, the VSOCK interface, and Go's HTTP stack in
    the enclave application.}
    \label{fig:stress-test}
\end{figure}

\begin{description}
  \item[Full:] This represents the full data flow as it would occur in
    production, i.e. client $\rightarrow$ TCP proxy $\rightarrow$ VSOCK
    interface $\rightarrow$ enclave application.

  \item[No proxy:] This setup does not contain the TCP proxy, i.e., the client
    talks to the VSOCK interface directly, i.e. client $\rightarrow$ VSOCK
    interface $\rightarrow$ enclave application.

  \item[Direct:] This setup does not contain the TCP proxy and the VSOCK
    interface.  Instead, the client directly talks to an application instance that is
    running \emph{outside} the enclave, i.e., client $\rightarrow$ application.
\end{description}

As part of our measurement setup, We first deploy the code from
Figure~\ref{fig:hello-world}---a minimalistic application that responds with
the string ``hello world'' upon receiving requests for /hello-world.  It's
important to use a minimalistic application because we're only interested in
the latency that is caused by the components \emph{before} a request reaches
the enclave application.

\subsection{End-to-end Latency}
\label{sec:end-to-end}

To simulate clients, we use the HTTP load test tool Baton~\cite{baton}.  We run
Baton on the parent EC2 instance and instruct it to send as many requests to the
TCP proxy as possible within 30 seconds, using 50 concurrent threads.  We had to
patch Baton's source code to add VSOCK support (to be able to send requests
directly to the enclave, via the VSOCK interface) and to log latency
percentiles.  Note that our measurements constitute a \emph{lower bound} of the
latency that is achievable.  Real-world applications will exhibit higher latency
because clients send their requests over the Internet (which adds considerable
networking latency) and the enclave application is likely to be more complex
(which adds computational latency).

Figure~\ref{fig:latency-msmts} illustrates the results for our three test
setups.  The full pipeline is able to sustain 7,500 requests per second, with a
mean latency of 12.7 milliseconds.  Removing the proxy increases the requests
to 14,100 per second and lowers the mean latency to 6.5.  Finally, a direct
connection to the application---without proxy and VSOCK interface--handles
27,900 requests per second, and a mean latency of only 3.2 milliseconds.

\begin{figure}[t]
    \centering
    \begin{tabular}{l r r r}
    \toprule
      Setup & Reqs/sec & Mean lat. (ms) & Max lat. (ms) \\
    \midrule
    Full & 7,500 & 12.7 & 56.0 \\
    No proxy & 14,100 & 6.5 & 52.0 \\
    Direct & 27,900 & 3.2 & 50.0 \\
    \bottomrule
    \end{tabular}
    \caption{Using 100 concurrent requests and 100,000 requests in total.}
    \label{fig:latency-msmts}
\end{figure}

Figure~\ref{fig:latency-cdf} shows the empirical CDF of the same latency
measurements for our three test setups.

\begin{figure}[t]
    \centering
    \input{sections/figures/latency-cdf}
    \label{fig:latency-cdf}
    \caption{The empirical CDF of the latency distributions of our three test setups.}
\end{figure}

\phw{The paper would benefit from a section on the framework's usability. How do
we quantify this?}

% \subsection{Operational Experience}
% \label{sec:operations}
%
%
% \begin{itemize}
%     \item We deployed application X on YYYY-MM-DD.
%
%     \item How many clients were involved?  How many requests per second did
%     they make?
%
%     \item We published a blog post.  Discuss user reception.
%
%     \item Discuss how useful we found the system in the context of
%     anti-fraud.
%
%     \item Discuss operational issues and gotchas.
% \end{itemize}
