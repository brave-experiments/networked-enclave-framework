\section{Evaluation}%
\label{sec:evaluation}

We now evaluate \tool{} with respect to security, financial cost, and
performance.  We ran all our performance measurements on a c5.xlarge EC2
instance~\cite{c5-instance}, which is on the lower end of enclave-capable
instance types.  Our performance numbers therefore represent a lower bound of
what's possible with Nitro Enclaves.  More powerful instance types will yield
better performance.

\subsection{Security considerations}%
\label{sec:security}

There are three key components to the overall security of a \tool{} application:
(i) Amazon's Nitro system itself,
(ii) \tool{}, and
(iii) the application running on top of \tool{}.

The security foundation lies in the soundness of the design of Nitro Enclaves.
While Amazon published the conceptual design~\cite{Bean2022a}, the concrete
hardware and software implementation is closed source.  The decision to
allocate physically separate resources to enclaves is sound but only time will
tell if Nitro Enclaves can resist the types of attacks that have been plaguing
SGX.

\Tool{}'s security reduces to the complexity of our code and the security of its
cryptographic building blocks.  These building blocks are SHA-256 (to hash
public key material that is embedded in the attestation document), the NaCl
cryptographic library~\cite{nacl} (to implement key
synchronization),\footnote{Specifically, we use NaCl's box API, which uses
Curve25519, XSalsa20, and Poly1305 to encrypt and authenticate messages.} TLS in
at least version 1.2 (to provide a secure channel between clients and the
enclave), and Go's CSPRNG, which is seeded with randomness from the Nitro
hypervisor.
% ls -1 *.go | grep -v "_test.go" | xargs wc -l
One measure of our code complexity is its size.  Excluding unit tests, \tool{}
counts less than 1,700 lines of code and has nine direct dependencies
that are not maintained by either us or the Go project.\footnote{Most
dependencies provide networking functionality like a user-space TCP stack, code
that provides a TAP interface, and a wrapper for Linux's netlink interface.}
%
Nine is worse than zero, but is still manageable and auditable in its entirety.
Our choice of using the memory-safe Go and the (comparatively) small trusted
computing base reduces---but does not eliminate!---the attack surface.

The highest layer in the software stack is the enclave application itself.  The
most significant security issues are side channel attacks and programming bugs.
It is the application developer's responsibility to prevent side channel attacks
and write bug-free code.  As we pointed out in Section~\ref{sec:limitations},
programming bugs can be intentional, i.e., the service provider may deliberately
introduce bugs that leak sensitive information.  From the user's point of view,
eternal vigilance is therefore the price of security.

\subsection{Financial cost}%
\label{sec:cost}

Nitro Enclaves do not incur any extra cost in addition to that of the underlying
EC2 host---they can be considered a ``free'' extension to EC2.  Nitro enclaves
are however only available for select types of EC2 instances because they
require their own CPU and a minimum amount of memory, and those instance types
are pricier than the lowest tier that AWS offers.  We tested all of the
practical applications of Section~\ref{sec:applications} on a c5.xlarge
instance, which is on the lower end of enclave-enabled EC2 instance types.  This
instance comes with four vCPUs and 8 GiB of memory.  As of March 2023, a
c5.xlarge instance costs USD 0.17 per hour, which amounts to approximately USD
125 per month.

\subsection{Attestation document request rate}%
\label{sec:attestation-performance}

The fetching of attestation documents is a critical part of our framework's
overall performance.  We built a stress test enclave application that runs a
busy loop for 60 seconds to request as many attestation documents as possible.
For each request, we ask the hypervisor to include an incrementing nonce in the
attestation document to eliminate any speedups by caching.
Figure~\ref{fig:att-perf} illustrates the results.  We were able to obtain
approximately 860 documents per second with the median request taking 1.1 ms to
complete.  99\% of requests finished in less than 1.31 ms.  Our experience from
building enclave applications (cf.~\S~\ref{sec:applications}) suggests that the
attestation document request rate is unlikely to be a bottleneck for real-world
deployments.

\begin{figure}[t]
    \centering
    \input{diagrams/attestation-document-request-latency/att-doc-latencies.tex}
    \caption{The latency distribution (as CDF) of requesting 51,821
    attestation documents from the Nitro hypervisor.}\label{fig:att-perf}
\end{figure}

\subsection{Application latency}%
\label{sec:latency}

We seek to measure two types of latency: (i) the latency induced by the
interface between an EC2 host and its Nitro Enclave and (ii) the latency induced
by \tool{}, both with and without \tool{}'s reverse proxy configuration.  The
former is outside our control while the latter is entirely controlled by us.  We
built a lightweight enclave application to measure these latencies.  The
application implements a Web server written in Go that responds with the string
``hello world'' upon receiving requests for its index page.  We made this
application minimal because we're only interested in the latency \emph{before} a
request reaches the enclave application.  For this reason, the Web server only
speaks the computationally inexpensive HTTP (instead of HTTPS).  To simulate
clients, we use the HTTP load test tool Baton~\cite{baton} in git commit
\texttt{576339}.  We patched Baton's source code to add VSOCK support (to send
requests directly to the enclave, via the VSOCK interface) and to log latency
percentiles.  Equipped with both an HTTP server and a client, we measure HTTP
request latencies for five setups:

\begin{description}
  \item[Loopback] The client talks to the Web server via the loopback interface.
    No enclave is involved.  This setup constitutes the latency baseline we
    compare against.

  \item[Enclave] The Web server runs inside a Nitro Enclave but \emph{without
    \tool{}}.  All traffic goes over the VSOCK interface.  This measures the
    latency that the Nitro Enclave's VSOCK interface introduces.

  \item[\Tool{}-nrp] The Web server runs inside a Nitro Enclave but without a
    reverse proxy.  The suffix ``nrp'' is short for ``no reverse proxy''.  This
    measures the latency introduced by \tool{}'s TAP forwarding code.

  \item[\Tool{}] The Web server runs inside a Nitro Enclave with \tool{} acting
    as a reverse HTTP proxy.  This measures the latency introduced by \tool{}'s
    TAP forwarding code \emph{and} its reverse HTTP proxy.
\end{description}

We run Baton on the parent EC2 host and instruct it to send 100,000 requests in
six sequential experiments, using 1, 5, 10, 25, 50, and 100 concurrent threads.
Note that our measurements are designed to measure the \emph{lower bound} for
latency.  Real-world applications will exhibit higher latency because clients
send their requests over the Internet (which adds considerable networking
latency) and the enclave application is likely to be more complex (which adds
computational latency).

\begin{figure}[t]
  \centering
  \input{diagrams/http-requests-per-sec/http-requests-per-sec.tex}
  \caption{The number of HTTP requests per second as requests are sent from an
  increasing number of threads.}%
  \label{fig:http-reqs-sec}
\end{figure}

Figure~\ref{fig:http-reqs-sec} illustrates the results.  In all setups, the
requests per second increase as the number of threads increases, but only up to
25 threads, at which point we see diminishing returns.  As expected, the
loopback interface---our baseline---performs the best, handling 41,000 requests
per second for 25 threads.  The Enclave setup can sustain approximately half of
that, namely 20,000 reqs/s.  Recall that the Enclave setup constitutes the
maximum achievable performance for \tool{}.

We find that both \tool{} and \tool{} without a reverse proxy (denoted as
\Tool{}-nrp) can sustain 2,300 and 2,600 reqs/s, respectively---13\% of what is
achievable over the enclave interface.  As expected, \tool{} performs better
without reverse proxy because less complexity is involved.  We attribute the
performance difference between \tool{} and the Enclave baseline to the
user-space TCP stack that our software dependency gVisor introduces.

Figure~\ref{fig:http-req-lat} shows how long it takes to answer the HTTP
requests we issued as part of this experiment.  The four charts show a latency
CDF for 1, 10, 50, and 100 threads, respectively.  We omitted charts for 5 and
25 threads because of page constraints.

\begin{figure*}[t]
  \centering
  \input{diagrams/http-request-latencies/http-request-latencies.tex}
  \caption{The round-trip time distributions (as CDF) of stress-testing an
  in-enclave Web server as the number of concurrent requesters increases from 1
  to 100 threads.}%
  \label{fig:http-req-lat}
\end{figure*}

\subsection{Application throughput}%
\label{sec:throughput}

Next, we measure the throughput that we can achieve over the VSOCK interface.
To that end, we use a VSOCK-enabled fork of the iperf3 performance measurement
tool in git commit \texttt{9245f9a}~\cite{iperf-vsock}.  iperf3 measures the
throughput of a networking link using a client/server model.  In our experiment,
we start an iperf3 server instance inside the enclave and the corresponding
client instance on the parent EC2 host.\footnote{The command that we ran on the
server was ``\texttt{iperf3 -{}-vsock -s}'' and on the client ``\texttt{iperf3
-{}-vsock -c 4}.''}  The client then talks to the server via the VSOCK interface
and determines the throughput.  Table~\ref{tab:iperf3} shows the results of this
experiment.  When running both the iperf3 client \emph{and} server on the EC2
host---which effectively measures the throughput of the EC2 host's loopback
interface---we achieve 57 GBit/s of throughput.  Running the iperf3 server
inside an enclave limits throughput to 3 Gbit/s while \tool{} results in
approximately 0.3 Gbit/s.  Iperf3 does not use HTTP and we therefore cannot
measure \tool{} in its reverse proxy mode.

\begin{table}[t]
    \centering
    \begin{tabular}{l r r}
    \toprule
      Setup & C $\rightarrow$ S (Gbits/s) & S $\rightarrow$ C (Gbit/s) \\
    \midrule
      Loopback        & 57.0 & 57.0 \\
      Enclave         &  3.6 &  3.2 \\
      Nitriding-nrp &  0.3 &  1.1 \\
    \bottomrule
    \end{tabular}
    \caption{The TCP throughput of running iperf3 over the
    loopback interface, inside an enclave, and inside an enclave using
    nitriding (without reverse proxy).}%
    \label{tab:iperf3}
\end{table}
