\section{Evaluation}%
\label{sec:evaluation}

We evaluate our enclave framework with respect to
security (\S~\ref{sec:security}),
financial cost (\S~\ref{sec:cost}), and performance.  As for performance, we
study the rate at which one can generate
attestation documents (\S~\ref{sec:attestation-performance}),
application latency (\S~\ref{sec:latency}),
and application throughput (\S~\ref{sec:throughput}).

We ran our performance measurements on a c5.xlarge EC2 host, which is on the
lower end of enclave-capable instance types.  Our performance numbers therefore
represent a lower bound of what's possible.  More powerful (and therefore more
expensive) instance types result in better performance.

\subsection{Security considerations}%
\label{sec:security}

\phw{Need to make this more rigorous.}

There are three key components to the overall security of enclave applications;
(i) Amazon's Nitro enclave system itself,
(ii) our framework, and
(iii) the application that runs on top of our framework.

The very foundation of our framework's security lies in the soundness of the
design of Nitro enclaves.  While Amazon published the conceptual design, the
concrete hardware and software implementation remains confidential.  The
decision to allocate physically separate resources to enclaves appears promising
but only time will tell if Nitro enclaves can resist the types of attacks that
have been plaguing SGX.  If we assume that Nitro enclaves are acceptably
secure, the next critical layer is our software framework.

A significant security aspect of our framework is its size; it is well
understood that complexity is the enemy of security.  Excluding unit tests, our
framework counts less than 1,300 lines of code and has four direct dependencies
that are not maintained by either us or the Go project.\footnote{The
dependencies are chi~\cite{chi} (provides an HTTP request router),
nsm~\cite{nsm} (provides an interface to interact with the Nitro hypervisor),
vsock~\cite{vsock} (provides an API for the VSOCK address family), and
tenus~\cite{tenus} (provides an API to configure Linux's networking devices).}
Four is worse than zero, but is still manageable and reasonably easy to audit in
its entirety.  We believe that our choice of using Go and the deliberately small
trusted computing base greatly reduces---but does not eliminate!---the attack
surface.

The highest layer in the software stack is the enclave application itself.  The
biggest security threat are side channel attacks and programming bugs---both
unintentional and intentional.  It is the application developer's
responsibility to prevent side channel attacks and write bug-free code.  As we
pointed out in Section~\ref{sec:limitations}, programming bugs can be
intentional, i.e., the service provider may deliberately introduce bugs that
leak sensitive information.  From the user's point of view, eternal vigilance
is therefore the price of security.

\subsection{Financial cost}%
\label{sec:cost}

Nitro enclaves do not incur any extra cost in addition to what the underlying
EC2 host costs---they can be considered a ``free'' extension to EC2.  Nitro
enclaves are however only available for select types of EC2 instances because
they require their own CPU and a minimum amount of memory, and those instance
types are pricier than the lowest tier that AWS offers.  We tested all of the
practical applications of Section~\ref{sec:applications} on a c5.xlarge
instance, which is on the lower end of enclave-enabled EC2 instance types.
This instance comes with four vCPUs and 8 GiB of memory.  As of March 2023, a
c5.xlarge instance costs USD 0.17 per hour, which amounts to approximately USD
125 per month.

\subsection{Attestation document request rate}%
\label{sec:attestation-performance}

The fetching of attestation documents is a critical part of our framework's
overall performance.  We built a stress test application that runs a busy loop
for 60 seconds to request as many attestation documents as possible.  For each
request, we ask the hypervisor to include an incrementing nonce in the
attestation document, to avoid any speedups by caching.  We ran this stress test
on a c5.xlarge EC2 host which comes with four CPUs and eight GiB of memory.
Table~\ref{tab:att-perf} shows the results.  We were able to obtain
approximately 840 documents per second with the median request taking 1.1
milliseconds to complete.

\begin{table}[t]
    \centering
    \begin{tabular}{r r r r}
    \toprule
      Min. (ms) & Median (ms) & Mean (ms) & Max. (ms) \\
    \midrule
      1.1 & 1.1 & 1.2 & 7.9 \\
    \bottomrule
    \end{tabular}
    \caption{Summary statistics capturing the expected latency of requesting
    attestation documents from the Nitro hypervisor.}%
    \label{tab:att-perf}
\end{table}

\subsection{Application latency}%
\label{sec:latency}

We seek to measure two types of latency: (i) the latency induced by the
interface between an EC2 host and its Nitro enclave and (ii) the latency induced
by \tool{}, both with and without \tool{}'s reverse proxy configuration.  We
built a lightweight enclave application that helps us measure these latencies.
The application implements a Web server that responds with the string ``hello
world'' upon receiving requests for its index.  We made this application minimal
because we're only interested in the latency \emph{before} a request reaches the
enclave application.  We therefore also deactivate the Web server's TLS support
and use plain HTTP for our measurements.  To simulate clients, we use the HTTP
load test tool Baton~\cite{baton}.  Equipped with both an HTTP server and
client, we measure the request latency to our application in five separate
setups:

\begin{description}
  \item[Loopback] The client talks to the Web server via the loopback interface.
    This setup provides the latency baseline that we compare against.

  \item[Docker] The Web server runs in a local Docker container.

  \item[Enclave] The Web server runs inside a Nitro enclave but without \tool{}.
    All traffic goes over the VSOCK interface.  This measures the latency
    introduced by the interface to the Nitro enclave.

  \item[\Tool{}-nrp] The Web server runs inside a Nitro enclave but without a
    reverse proxy.  This measures the latency introduced by \tool{}'s tap
    forwarding code.

  \item[\Tool{}] The Web server runs inside a Nitro enclave with \tool{} acting
    as a reverse HTTP proxy.  This measures the latency introduced by \tool{}'s
    tap forwarding code \emph{and} its reverse HTTP proxy.
\end{description}

We run
Baton on the parent EC2 host and instruct it to send 100,000 requests using 10
concurrent threads.  We had to patch Baton's source code to add VSOCK support
(to be able to send requests directly to the enclave via the VSOCK interface)
and to log latency percentiles.  Note that our measurements constitute a
\emph{lower bound} of the latency that is achievable.  Real-world applications
will exhibit higher latency because clients send their requests over the
Internet (which adds considerable networking latency) and the enclave
application is likely to be more complex (which adds computational latency).

Figure~\ref{fig:latency-msmts} illustrates the results for our three test
setups.  The full pipeline is able to sustain 7,500 requests per second, with a
mean latency of 12.7 milliseconds.  Removing the proxy nearly doubles the
requests to 14,100 per second and lowers the mean latency to 6.5.  Finally, a
direct connection to the application---without proxy and VSOCK interface--once
again nearly doubles the number of requests, reaching 27,900 per second, with a
mean latency of only 3.2 milliseconds.  Figure~\ref{fig:latency-cdf} shows the
empirical CDF of the same latency measurements for our three test setups.

\begin{table}[t]
    \centering
    \begin{tabular}{l r r}
    \toprule
      Setup & C $\rightarrow$ S (Gbits/sec) & S $\rightarrow$ C (Gbit/sec) \\
    \midrule
      Loopback        & 57.0 & 57.0 \\
      Docker          & 26.0 & 28.9 \\
      Enclave         &  3.6 &  3.2 \\
      Nitriding(-nrp) &  0.3 &  1.1 \\
    \bottomrule
    \end{tabular}
    \caption{The TCP throughput measurements when running iperf3 over the
    loopback interface, in Docker, inside an enclave, or inside an enclave using
    nitriding.}%
    \label{fig:iperf3}
\end{table}

\begin{table}[t]
    \centering
    \begin{tabular}{l r r r}
    \toprule
      Setup & \makecell[r]{Requests\\(per sec)} &
              \makecell[r]{Mean latency\\(ms)} &
              \makecell[r]{Max latency\\(ms)} \\
    \midrule
      Loopback      & 40,913 &  0.06 & 20 \\
      Docker        & 19,321 &  0.12 & 15 \\
      Enclave       & 16,466 &  0.06 &  9 \\
      Nitriding-nrp &  2,288 &  3.74 & 53 \\
      Nitriding     &  1,214 &  7.63 & 63 \\
    \bottomrule
    \end{tabular}
    \caption{The number of HTTP requests per second when making requests from
    ten threads in parallel, and 100,000 requests in total.}%
    \label{fig:latency-msmts}
\end{table}

% \begin{figure}[t]
%     \centering
%     \begin{tabular}{l r r r}
%     \toprule
%       Setup & Reqs/sec & Mean lat. (ms) & Max lat. (ms) \\
%     \midrule
%     Full     &  7,500 & 12.7 & 56.0 \\
%     No proxy & 14,100 &  6.5 & 52.0 \\
%     Direct   & 27,900 &  3.2 & 50.0 \\
%     \bottomrule
%     \end{tabular}
%     \caption{Using 100 concurrent requests and 100,000 requests in total.}
%     \label{fig:latency-msmts}
% \end{figure}

\begin{figure}[t]
    \centering
    \input{diagrams/latency-cdf/latency-cdf.tex}
    \caption{The empirical CDF of the latency distributions of our three test
      setups.}\label{fig:latency-cdf}
\end{figure}

\subsection{Application throughput}%
\label{sec:throughput}

Next, we measure the throughput that we can achieve over the VSOCK interface.
To that end, we use a VSOCK-enabled fork of the iperf3 performance measurement
tool in git commit ~\cite{iperf-vsock}.  iperf3 measures the throughput of a networking link
using a client/server model.  In our experiment, we start an iperf3 server
instance inside the enclave and the corresponding client instance on the parent
EC2 host.\footnote{The command that we ran on the server was
``\texttt{iperf3 -{}-vsock -s}'' and on the client ``\texttt{iperf3 -{}-vsock -c
4}.''} The client then talks to the server via the VSOCK interface and
determines the maximum possible throughput.  In this setup, iperf3 measured a
throughput of 4.09 GBit/s.  For comparison, when running both the iperf3 client
\emph{and} server on the EC2 host---which effectively measures the
throughput of the EC2 host's loopback interface---we achieve 55.5 GBit/s of
throughput.

To develop intuition on the perceived network performance of Nitro enclaves, we
built an enclave application that acts as a SOCKS proxy.  We then configured a
browser to use this enclave-enabled SOCKS proxy and browsed HD videos on
YouTube.  We found that the experience was seamless: videos loaded quickly,
played smoothly, and there was no perceivable latency impact when browsing the
Web.  We believe that the high throughput and low latency, coupled with this
anecdotal user experience report suggests that our framework is suitable for
demanding and latency-sensitive networking applications.
